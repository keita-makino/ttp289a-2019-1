import Regression from '../components/Regression';
import data from '../data/data.json';
import getValidResponses from '../utils/getValidResponses';
import Table from '../components/Table';
import GetPVal from '../components/GetPVal';

export const variable1 = [
  'CHILDLT6',
  'EFACT1',
  'EFACT7',
  'COMMTIME',
  'EFACT9',
  'JOBHOME',
  'MANCONST',
  'JOBCONST',
  'TECHCONSB',
  'CSO9FT2',
  'AGE',
  'INCOM'
];

### First Models

After the removal and re-classification of variables, we have evaluated the first models (male: FMA model, female: FFE model) and obtained the following results.

<Regression
  type={'logit'}
  data={getValidResponses(variable1)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 0)}
  input={variable1.map(item => ({name: item, location:0, type: "ASV"}))}
  table={{ title: 'Table 6. First Male Model' }}
  outPlot={{ title: 'Figure 5. Estimate by First Male Model' }}
/>
<Regression
  type={'logit'}
  data={getValidResponses(variable1)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 1)}
  input={variable1.map(item => ({name: item, location:0, type: "ASV"}))}
  table={{ title: 'Table 7. First Female Model' }}
  outPlot={{ title: 'Figure 6. Estimate by First Female Model' }}
/>

The result of the first two models somehow show a different implication. For example, the effect of having small children and commute stress are significant in the FFE model, whereas it does not really affect the telecommuting status of male workers. On the other hand, the office discipline exhibits a significance only in the FMA model. We assume that those which insignificant in one model can be removed for the better performance.

Also, looking at the tables, family / community directed, internal control, technology constraint and income have a significance in neither of the model. We can omit these variables from both of them to further improve the performance of each model.

### Second Models

Therefore, the second model for male and female workers, namely SMA and SFE model should be the following structure.

export const variable2 = ['COMMTIME', 'JOBHOME', 'MANCONST', 'CSO9FT2', 'AGE'];

export const variable3 = [
  'CHILDLT6',
  'COMMTIME',
  'EFACT9',
  'JOBHOME',
  'MANCONST',
  'JOBCONST',
  'AGE'
];

<Regression
  type={'logit'}
  data={getValidResponses(variable2)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 0)}
  input={variable2.map(item => ({name: item, location:0, type: "ASV"}))}
  table={{ title: 'Table 8. Second Male Model' }}
  outPlot={{ title: 'Figure 7. Estimate by Second Male Model' }}
/>
<Regression
  type={'logit'}
  data={getValidResponses(variable3)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 1)}
  input={variable3.map(item => ({name: item, location:0, type: "ASV"}))}
  table={{ title: 'Table 9. Second Female Model' }}
  outPlot={{ title: 'Figure 8. Estimate by Second Female Model' }}
/>

Now all the variable of SFE model become significant. So we identify this as the final model for female workers. Looking at the t-statistics, it seems that the most important positive factor is whether the participant has some part of job that can be done at home. This is quite reasonable and intuitive. The lack of manager support has the second largest magnitude of importance, but it works as a negative factor, which is also rational. As the third group, commute time, commute stress, job unsuitability, and age has some effect to the outcome. The latter three factors are intuitive, but interestingly, the commute time has a negative effect, which could be somehow irrational. Those who live close to workplace would bring their work to home in a short-break time, which might be counted as a kind of telecommuting. But there should be some additional investigation.

### Third Models

Again, in the SMA model the age variable become insignificant. Then, the third model for male workers (TMA model) will be without this variable. On the other hand, the second female model would be the best model as is.

export const variable4 = ['COMMTIME', 'JOBHOME', 'MANCONST', 'CSO9FT2'];

<Regression
  type={'logit'}
  data={getValidResponses(variable4)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 0)}
  input={variable4.map(item => ({ name: item, location: 0, type: 'ASV' }))}
  table={{ title: 'Table 10. Third Male Model' }}
  outPlot={{ title: 'Figure 9. Estimate by Third Male Model' }}
/>

The model for male workers has less variables than that for female workers. Instead, the effect of availability of job at home is an overwhelmingly strong decision factor. However, looking at the distribution of this variable (Figure 4.8.), most of the participant have some job that can be done at home. Therefore, the coefficient of all the variables become negative value to offset this unbalanced value in the sample.

### Statistical Test

Let's examine if the TMA model and SFE model are equally good as the FMA model and FFE model. If there is no significance between each pair of those models, we can say that the restricted models are better. First, we evaluate the first models using the same dataset as the final models.

<Regression
hideDetails={true}
  type={'logit'}
  data={getValidResponses(variable4)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 0)}
  input={variable1.map(item => ({ name: item, location: 0, type: 'ASV' }))}
  table={{ title: 'Table 11. First Male Model (with dataset for TMA)' }}
  outPlot={{ title: 'Figure 10. Estimate by First Male Model' }}
/>
<Regression
hideDetails={true}
  type={'logit'}
  data={getValidResponses(variable3)
    .map(item => data[item])
    .filter(item => item['FEMALE'] === 1)}
  input={variable1.map(item => ({ name: item, location: 0, type: 'ASV' }))}
  table={{ title: 'Table 12. First Female Model (with dataset for SFE)' }}
  outPlot={{
    title: 'Figure 11. Estimate by First Female Model (with dataset for SFE)'
  }}
/>

Under the null hypothesis that there is no significant difference between the models, the log-likelihood ratios will respectively follow a chi-squared distribution with degree of freedom $13 - 5 = 8$ (male) and $13 - 8 = 5$ (female). Then,

<GetPVal x={6.888} k={8} />

$$
LR_M = -2(\mathcal{L}_{TMA} - \mathcal{L}_{FMA}) = -2(-77.059 + 73.615) = 6.888 \\
\text{(p-value)} = 0.549
$$

<GetPVal x={2.956} k={5} />

$$
LR_F = -2(\mathcal{L}_{SFE} - \mathcal{L}_{FFE}) = -2(-46.066 + 44.588) = 2.956 \\
\text{(p-value)} = 0.707
$$

Therefore, we can conclude that the models with fewer variables are better.

### Summary of Hypotheses in variables

In the previous section, we have defined several hypotheses regarding how each variable would behave in different gender groups. The table below summarize our judgement about them based on the model assessment.

<Table />
